{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# для определения тематики книги по названию\n",
    "\n",
    "one-to-all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import words, stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, ISRIStemmer, LancasterStemmer, PorterStemmer, RegexpStemmer, RSLPStemmer, SnowballStemmer\n",
    "from nltk.corpus import words as nltk_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import *\n",
    "from sklearn.utils import resample # будем делать downsumpling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blacat\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "#w2v_model = Word2Vec.load('enwv\\en.model')\n",
    "w2v_model = KeyedVectors.load_word2vec_format('enwv\\GoogleNews-vectors-negative300.bin', binary= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blacat\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding, Dropout, Conv1D, Dense, Flatten, LSTM, Bidirectional\n",
    "from keras.layers.pooling import GlobalMaxPool1D, MaxPooling1D, MaxPool1D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#расширим список стоп-слов\n",
    "## встречаются буквы по отдельности (окочания после апострофа и тп), возможно ошибки написания\n",
    "mystopwords = set(stopwords.words('english') + ['a', 'b', 'c', 'd', 'e', 'f', 'j', 'h', \n",
    "           'g', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', \n",
    "           's', 't', 'u', 'v', 'w', 'x', 'y', 'z']) \n",
    "\n",
    "eng_let = ['a', 'b', 'c', 'd', 'e', 'f', 'j', 'h', 'i', \n",
    "           'g', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', \n",
    "           's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemm = WordNetLemmatizer()\n",
    "def tokenize(x):\n",
    "    article = re.compile(u'[a-z]+')    \n",
    "    return article.findall(x)\n",
    "\n",
    "def stop_words(x):\n",
    "    return [token for token in x if not token in mystopwords]\n",
    "\n",
    "def lemma(x):\n",
    "    return [lemm.lemmatize(token) for token in x ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blacat\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (2,5,7,9,11,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "books = pd.read_csv('title.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58751"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allbooks = len(books)\n",
    "allbooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subj_list = ['children', 'fantasy','medicine', 'music','mystery and detective stories', \n",
    "             'recipes', 'religion', 'romance', 'science','science fiction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'children': 4235.0,\n",
       " 'fantasy': 4384.0,\n",
       " 'medicine': 3144.0,\n",
       " 'music': 6077.0,\n",
       " 'mystery and detective stories': 2316.0,\n",
       " 'recipes': 1225.0,\n",
       " 'religion': 11098.0,\n",
       " 'romance': 10181.0,\n",
       " 'science': 13222.0,\n",
       " 'science fiction': 3770.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# число книг с определенной темой\n",
    "b_num = {}\n",
    "for i in subj_list:\n",
    "    books[i].sum()\n",
    "    b_num[i] = books[i].sum()\n",
    "b_num\n",
    "\n",
    "#subj_sum = books.religion.sum()\n",
    "#subj_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 58751 entries, 0 to 58750\n",
      "Data columns (total 10 columns):\n",
      "children                         58751 non-null float64\n",
      "fantasy                          58751 non-null float64\n",
      "medicine                         58751 non-null float64\n",
      "music                            58751 non-null float64\n",
      "mystery and detective stories    58751 non-null float64\n",
      "recipes                          58751 non-null float64\n",
      "religion                         58751 non-null float64\n",
      "romance                          58751 non-null float64\n",
      "science                          58751 non-null float64\n",
      "science fiction                  58751 non-null float64\n",
      "dtypes: float64(10)\n",
      "memory usage: 4.5 MB\n"
     ]
    }
   ],
   "source": [
    "books[subj_list].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>children</th>\n",
       "      <th>fantasy</th>\n",
       "      <th>medicine</th>\n",
       "      <th>music</th>\n",
       "      <th>mystery and detective stories</th>\n",
       "      <th>recipes</th>\n",
       "      <th>religion</th>\n",
       "      <th>romance</th>\n",
       "      <th>science</th>\n",
       "      <th>science fiction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>58751.000000</td>\n",
       "      <td>58751.000000</td>\n",
       "      <td>58751.000000</td>\n",
       "      <td>58751.000000</td>\n",
       "      <td>58751.000000</td>\n",
       "      <td>58751.000000</td>\n",
       "      <td>58751.000000</td>\n",
       "      <td>58751.000000</td>\n",
       "      <td>58751.000000</td>\n",
       "      <td>58751.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.072084</td>\n",
       "      <td>0.074620</td>\n",
       "      <td>0.053514</td>\n",
       "      <td>0.103437</td>\n",
       "      <td>0.039421</td>\n",
       "      <td>0.020851</td>\n",
       "      <td>0.188899</td>\n",
       "      <td>0.173291</td>\n",
       "      <td>0.225051</td>\n",
       "      <td>0.064169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.258629</td>\n",
       "      <td>0.262779</td>\n",
       "      <td>0.225058</td>\n",
       "      <td>0.304531</td>\n",
       "      <td>0.194595</td>\n",
       "      <td>0.142886</td>\n",
       "      <td>0.391432</td>\n",
       "      <td>0.378502</td>\n",
       "      <td>0.417620</td>\n",
       "      <td>0.245056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           children       fantasy      medicine         music  \\\n",
       "count  58751.000000  58751.000000  58751.000000  58751.000000   \n",
       "mean       0.072084      0.074620      0.053514      0.103437   \n",
       "std        0.258629      0.262779      0.225058      0.304531   \n",
       "min        0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       mystery and detective stories       recipes      religion  \\\n",
       "count                   58751.000000  58751.000000  58751.000000   \n",
       "mean                        0.039421      0.020851      0.188899   \n",
       "std                         0.194595      0.142886      0.391432   \n",
       "min                         0.000000      0.000000      0.000000   \n",
       "25%                         0.000000      0.000000      0.000000   \n",
       "50%                         0.000000      0.000000      0.000000   \n",
       "75%                         0.000000      0.000000      0.000000   \n",
       "max                         1.000000      1.000000      1.000000   \n",
       "\n",
       "            romance       science  science fiction  \n",
       "count  58751.000000  58751.000000     58751.000000  \n",
       "mean       0.173291      0.225051         0.064169  \n",
       "std        0.378502      0.417620         0.245056  \n",
       "min        0.000000      0.000000         0.000000  \n",
       "25%        0.000000      0.000000         0.000000  \n",
       "50%        0.000000      0.000000         0.000000  \n",
       "75%        0.000000      0.000000         0.000000  \n",
       "max        1.000000      1.000000         1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[subj_list].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "books.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "children                          4235.0\n",
       "fantasy                           4384.0\n",
       "medicine                          3144.0\n",
       "music                             6077.0\n",
       "mystery and detective stories     2316.0\n",
       "recipes                           1225.0\n",
       "religion                         11098.0\n",
       "romance                          10181.0\n",
       "science                          13222.0\n",
       "science fiction                   3770.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[subj_list].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = books['title']\n",
    "y = books[subj_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, ytr, ytst = train_test_split(X, y, test_size=0.2, random_state=888, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11751"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All words: 26968 . Max title length: 41\n",
      "Vocabular size: 26968\n"
     ]
    }
   ],
   "source": [
    "counter = collections.Counter()\n",
    "maxlen = 0    \n",
    "for key, value in X.items():\n",
    "    title_words = stop_words(lemma(tokenize(value)))\n",
    "    len_title = len(title_words)\n",
    "    for w in title_words:\n",
    "        counter[w] += 1\n",
    "    if len_title>maxlen:\n",
    "        maxlen = len_title\n",
    "\n",
    "all_w = len(counter) # число всех слов в названиях книг\n",
    "print('All words: ' + str(all_w) ,'. Max title length: '+ str(maxlen))\n",
    "    \n",
    "VOCAB_SIZE = all_w #используем все слова для создания эмбеддингов для лучшего результата. \n",
    "EMBED_SIZE = 300\n",
    "NUM_FILTERS = 256\n",
    "NUM_WORDS = 3\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20\n",
    "    \n",
    "word2index = collections.defaultdict(int)\n",
    "for wid, word in enumerate(counter.most_common(VOCAB_SIZE)):\n",
    "    word2index[word[0]] = wid\n",
    "vocab_sz = len(word2index) \n",
    "index2word = {v:k for k, v in word2index.items()}\n",
    "        \n",
    "\n",
    "print('Vocabular size: '+ str(vocab_sz))\n",
    "    \n",
    "embedd_wts = np.zeros((vocab_sz, EMBED_SIZE))\n",
    "for word, index in word2index.items():\n",
    "        #print(word, index)\n",
    "    try:\n",
    "        embedd_wts[index, :] = w2v_model[word]\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    \n",
    "Xtrain, Xtest = [], []\n",
    "for i in X_train:\n",
    "    title_words = stop_words(lemma(tokenize(i)))\n",
    "    wids = [word2index[word] for word in title_words]\n",
    "    Xtrain.append(wids)\n",
    "    \n",
    "for i in X_test:\n",
    "    title_words = stop_words(lemma(tokenize(i)))\n",
    "    wids = [word2index[word] for word in title_words]\n",
    "    Xtest.append(wids)\n",
    "        \n",
    "Xtr = np.array(pad_sequences(Xtrain , maxlen = maxlen))\n",
    "Xtst = np.array(pad_sequences(Xtest, maxlen = maxlen)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализация нейронной сети\n",
    "\n",
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = Sequential()\n",
    "md.add(Embedding(vocab_sz, EMBED_SIZE, input_length=maxlen, weights = [embedd_wts], trainable=False))\n",
    "md.add(LSTM(256, input_shape=(vocab_sz, EMBED_SIZE),  activation= 'relu' ))\n",
    "md.add(Dropout(0.15))\n",
    "#md.add(LSTM(128 , return_sequences=True , activation= 'relu' ))\n",
    "#md.add(Dropout(0.15))\n",
    "#md.add(LSTM(64, activation= 'relu' ))\n",
    "#md.add(Dropout(0.15))\n",
    "md.add(Dense(10, activation= 'softmax'))\n",
    "#optimizer = RMSprop(1e-4)\n",
    "md.compile(optimizer= 'adam', loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "checkp = ModelCheckpoint(filepath='title_weights\\weights.best.title.hdf5', \n",
    "                               verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39950 samples, validate on 7050 samples\n",
      "Epoch 1/10\n",
      "39936/39950 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.9549Epoch 00001: val_loss did not improve\n",
      "39950/39950 [==============================] - 161s 4ms/step - loss: 0.1215 - acc: 0.9549 - val_loss: 0.1614 - val_acc: 0.9426\n",
      "Epoch 2/10\n",
      "39936/39950 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9544Epoch 00002: val_loss did not improve\n",
      "39950/39950 [==============================] - 160s 4ms/step - loss: 0.1230 - acc: 0.9544 - val_loss: 0.1620 - val_acc: 0.9425\n",
      "Epoch 3/10\n",
      "39936/39950 [============================>.] - ETA: 0s - loss: 0.1522 - acc: 0.9536Epoch 00003: val_loss did not improve\n",
      "39950/39950 [==============================] - 165s 4ms/step - loss: 0.1522 - acc: 0.9536 - val_loss: 0.1630 - val_acc: 0.9430\n",
      "Epoch 4/10\n",
      "39936/39950 [============================>.] - ETA: 0s - loss: 0.1174 - acc: 0.9564Epoch 00004: val_loss did not improve\n",
      "39950/39950 [==============================] - 168s 4ms/step - loss: 0.1174 - acc: 0.9564 - val_loss: 0.1636 - val_acc: 0.9422\n",
      "Epoch 5/10\n",
      "39936/39950 [============================>.] - ETA: 0s - loss: 0.1174 - acc: 0.9565Epoch 00005: val_loss did not improve\n",
      "39950/39950 [==============================] - 160s 4ms/step - loss: 0.1174 - acc: 0.9565 - val_loss: 0.1625 - val_acc: 0.9436\n",
      "Epoch 6/10\n",
      "39936/39950 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9586Epoch 00006: val_loss did not improve\n",
      "39950/39950 [==============================] - 161s 4ms/step - loss: 0.1122 - acc: 0.9586 - val_loss: 0.1642 - val_acc: 0.9430\n",
      "Epoch 7/10\n",
      "39936/39950 [============================>.] - ETA: 0s - loss: 0.1105 - acc: 0.9593Epoch 00007: val_loss did not improve\n",
      "39950/39950 [==============================] - 159s 4ms/step - loss: 0.1105 - acc: 0.9593 - val_loss: 0.1666 - val_acc: 0.9429\n",
      "Epoch 8/10\n",
      "39936/39950 [============================>.] - ETA: 0s - loss: 0.1087 - acc: 0.9598Epoch 00008: val_loss did not improve\n",
      "39950/39950 [==============================] - 159s 4ms/step - loss: 0.1087 - acc: 0.9598 - val_loss: 0.1668 - val_acc: 0.9428\n",
      "Epoch 9/10\n",
      "39936/39950 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9607Epoch 00009: val_loss did not improve\n",
      "39950/39950 [==============================] - 165s 4ms/step - loss: 0.1057 - acc: 0.9607 - val_loss: 0.1730 - val_acc: 0.9420\n",
      "Epoch 10/10\n",
      "39936/39950 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9618Epoch 00010: val_loss did not improve\n",
      "39950/39950 [==============================] - 165s 4ms/step - loss: 0.1033 - acc: 0.9618 - val_loss: 0.1695 - val_acc: 0.9428\n"
     ]
    }
   ],
   "source": [
    "process = md.fit(Xtr, ytr, batch_size = 512,epochs=10, validation_split=0.15, callbacks=[checkp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11751/11751 [==============================] - 18s 2ms/step\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = process.history\n",
    "classes = process.model.predict_classes(Xtst, batch_size = 128, verbose = 1)\n",
    "proba = process.model.predict_proba(Xtst, batch_size = 128)\n",
    "print();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.46      0.59       850\n",
      "          1       0.61      0.26      0.36       853\n",
      "          2       0.76      0.57      0.65       619\n",
      "          3       0.90      0.60      0.72      1206\n",
      "          4       0.79      0.40      0.53       484\n",
      "          5       0.95      0.78      0.85       273\n",
      "          6       0.81      0.67      0.74      2243\n",
      "          7       0.71      0.67      0.69      1985\n",
      "          8       0.90      0.74      0.81      2625\n",
      "          9       0.65      0.18      0.28       770\n",
      "\n",
      "avg / total       0.80      0.59      0.67     11908\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pr = np.around(proba)\n",
    "pr[:20]\n",
    "report = classification_report(ytst, pr)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.39      0.54       850\n",
      "          1       0.61      0.19      0.30       853\n",
      "          2       0.79      0.48      0.60       619\n",
      "          3       0.91      0.59      0.72      1206\n",
      "          4       0.84      0.30      0.44       484\n",
      "          5       0.93      0.68      0.79       273\n",
      "          6       0.85      0.63      0.72      2243\n",
      "          7       0.82      0.49      0.61      1985\n",
      "          8       0.86      0.76      0.81      2625\n",
      "          9       0.71      0.06      0.11       770\n",
      "\n",
      "avg / total       0.83      0.53      0.62     11908\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(ytst, pr)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = (11, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = {}\n",
    "tpr = {}\n",
    "thr = {}\n",
    "roc_auc = {}\n",
    "#for i in range(0, 10):\n",
    "#    fpr[i] , tpr[i] , thr[i] = roc_curve(ytst[i],proba[i])\n",
    "#    roc_auc[i] = auc( fpr[i], tpr[i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for subj in subj_list:\n",
    "    plt.plot( fpr[subj], tpr[subj], label=subj +'. Area = {:.2f}'.format( roc_auc[subj]))       \n",
    "    \n",
    "plt.plot([0, 1], [0, 1])\n",
    "plt.xlim([-0.1, 1.1])\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC-AUC. Title Classificator')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for subj in subj_list:\n",
    "    plt.plot(np.array(range(1, 36)), history_subj[subj]['loss'], \\\n",
    "             label= subj )\n",
    " \n",
    "plt.title('Loss along Еpoch, distinct Subjects')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.grid()\n",
    "fig.savefig('Loss along Еpoch LSTM', dpi =300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for subj in subj_list:\n",
    "    plt.plot(np.array(range(1, 36)), history_subj[subj]['acc'], \\\n",
    "             label=subj)\n",
    "plt.title('Accuracy along Epoch, distinct Subjects')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
